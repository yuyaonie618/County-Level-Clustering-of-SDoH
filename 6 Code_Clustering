#gap statistic 
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt


Final_Variables = pd.read_excel("Final_Variables.xlsx", index_col="FIPS")

def gap_statistic(X, ks=range(1,11), n_refs=10, random_state=42):
    np.random.seed(random_state)
    gaps = []
    Wks = []
    Wkbs = []

    shape = X.shape
    mins = X.min(axis=0)
    maxs = X.max(axis=0)

    for k in ks:
        km = KMeans(n_clusters=k, n_init=10, random_state=random_state)
        km.fit(X)
        Wk = km.inertia_
        Wks.append(Wk)

        Wkb = []
        for _ in range(n_refs):
            X_ref = np.random.uniform(mins, maxs, size=shape)
            km_ref = KMeans(n_clusters=k, n_init=10, random_state=random_state)
            km_ref.fit(X_ref)
            Wkb.append(km_ref.inertia_)
        Wkbs.append(np.mean(Wkb))
        gaps.append(np.log(np.mean(Wkb)) - np.log(Wk))

    return np.array(gaps), np.array(Wks), np.array(Wkbs)

X = Final_Variables.values  
gaps, Wks, Wkbs = gap_statistic(X, ks=range(1,11), n_refs=10)
import matplotlib.pyplot as plt
plt.plot(range(1,11), gaps, marker='o')
plt.xlabel("Number of clusters K")
plt.ylabel("Gap Statistic")
plt.show()

best_k = np.argmax(gaps) + 1
print("Best K by Gap Statistic:", best_k)

#CH and DB index from 3 to 7 
from sklearn.cluster import KMeans
from sklearn.metrics import calinski_harabasz_score, davies_bouldin_score

ks = range(3, 8)  # 3 to 7
ch_scores = {}
db_scores = {}

for k in ks:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=20)
    
    # Fit and assign cluster
    Final_Variables[f"Cluster_{k}"] = kmeans.fit_predict(Final_Variables.drop(columns=[col for col in Final_Variables.columns if "Cluster_" in col], errors="ignore"))
    
    labels = Final_Variables[f"Cluster_{k}"]
    X = Final_Variables.drop(columns=[col for col in Final_Variables.columns if "Cluster_" in col])

    ch_scores[k] = calinski_harabasz_score(X, labels)
    db_scores[k] = davies_bouldin_score(X, labels)

print("Calinski–Harabasz:")
for k,v in ch_scores.items():
    print(f"K={k}: {v}")

print("\nDavies–Bouldin:")
for k,v in db_scores.items():
    print(f"K={k}: {v}")

#read the data again and do k means 
Final_Variables = pd.read_excel("Final_Variables.xlsx", index_col="FIPS")
kmeans_three = KMeans(n_clusters=3, random_state=42)
Final_Variables['Cluster_3'] = kmeans_three.fit_predict(Final_Variables)
# Count number of counties in each cluster
cluster_counts = Final_Variables["Cluster_3"].value_counts().sort_index()
print(cluster_counts)

#find the centroid 
feature_cols = Final_Variables.columns.drop(["Cluster_3"])

centroid_df = pd.DataFrame(kmeans_three.cluster_centers_, columns=feature_cols)
centroid_df["Cluster_3"] = range(3)

print(centroid_df)  # 6 x 76 (75 features + cluster)

#determine the variance of centroid 

centroid_df = centroid_df.drop(columns=["Cluster_3"], errors="ignore")

var_np = np.var(centroid_df.values, axis=0, ddof=1)

var_series = pd.Series(var_np, index=centroid_df.columns)

pd.set_option('display.max_rows', None)

# sort
var_series.sort_values(ascending=False)


#within-cluster, between-cluster, R^2
def compute_variance_decomposition_kmeans(df, cluster_col="Cluster_3"):
    
    # Numeric variables (exclude cluster label and FIPS if present)
    variables = df.columns.drop([cluster_col], errors='ignore')
    
    results = []

    for var in variables:
        x = df[var]
        
        # Total variance
        total_var = x.var(ddof=1)

        # Within-cluster variance
        within_var = 0
        n_total = len(df)

        for k, group in df.groupby(cluster_col):
            n_k = len(group)
            var_k = group[var].var(ddof=1)
            within_var += (n_k / n_total) * var_k   # weighted within variance

        # Between-cluster variance
        between_var = total_var - within_var

        # Weighted within-cluster standard deviation
        within_std = np.sqrt(within_var)

        results.append({
            "Variable": var,
            "TotalVariance": total_var,
            "WithinVariance": within_var,
            "BetweenVariance": between_var,
            "WithinStd": within_std,
            "Within/Total": within_var / total_var if total_var != 0 else np.nan,
            "Between/Total": between_var / total_var if total_var != 0 else np.nan
        })

    return pd.DataFrame(results).sort_values("Between/Total", ascending=False)

variance_table = compute_variance_decomposition_kmeans(Final_Variables, cluster_col="Cluster_3")
variance_table_sorted = variance_table.sort_values("Between/Total", ascending=False)
print(variance_table_sorted.head(20))
R2_cluster = variance_table["BetweenVariance"].sum() / variance_table["TotalVariance"].sum() 
print(f"\nOverall fraction of variance explained by clustering (R^2_cluster): {R2_cluster:.4f}")


#main contributor mean 
selected_vars = [
    "HEALTHCARE_PROVIDER_SPECIALTY_PPCA1",
    "HEALTHCARE_STRUCTURAL_PROVIDER_PPCA1",
    "HealthOutcomes_PPCA1",
    "Education_PPCA1",
    "HEALTH_BEH_BAD_PPCA1",
    "Income_Poverty_PPCA1",
    "Chronic_PPCA1",
    "HEALTHCARE_INSURANCE_PPCA1",
    "MaternalChild",
    "Environment_PPCA1",
    "MaternalChild"
]

# Compute cluster-wise mean
cluster_means = Final_Variables.groupby("Cluster_3")[selected_vars].mean()

print(cluster_means)

#main contributor mean and std 
selected_vars = [
    "HEALTHCARE_PROVIDER_SPECIALTY_PPCA1",
    "HEALTHCARE_STRUCTURAL_PROVIDER_PPCA1",
    "HealthOutcomes_PPCA1",
    "Education_PPCA1",
    "HEALTH_BEH_BAD_PPCA1",
]

# Compute cluster-wise mean and std
cluster_stats = Final_Variables.groupby("Cluster_3")[selected_vars].agg(['mean', 'std'])

print(cluster_stats)

import matplotlib.pyplot as plt
import seaborn as sns
#histogram of main contributor within cluster 
vars_to_plot = [
    "HEALTHCARE_PROVIDER_SPECIALTY_PPCA1",
    "HEALTHCARE_STRUCTURAL_PROVIDER_PPCA1",
    "HealthOutcomes_PPCA1",
    "Education_PPCA1",
    "HEALTH_BEH_BAD_PPCA1"
]

for var in vars_to_plot:
    plt.figure(figsize=(8,5))
    
    sns.histplot(
        data=Final_Variables, 
        x=var, 
        hue="Cluster_3",   
        multiple="stack",  
        kde=False,
        palette="Set2"
    )
    
    clusters = Final_Variables["Cluster_3"].unique()
    colors = sns.color_palette("Set2", n_colors=len(clusters))
    for c, color in zip(clusters, colors):
        median_val = Final_Variables[Final_Variables["Cluster_3"] == c][var].median()
        plt.axvline(
            median_val, 
            color=color, 
            linestyle='dashed', 
            linewidth=2, 
            label=f'Cluster {c} Median: {median_val:.2f}'
        )
    
    plt.title(f'Distribution of {var} by Cluster')
    plt.xlabel(var)
    plt.ylabel('Count')
    plt.legend()
    plt.show()

import pandas as pd
from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score
#compute similarity between two clustering methods 
df = pd.read_excel("K_Means_Label.xlsx")

labels_kmeans = df['Cluster_3']
labels_hier = df['HierCluster-3']

# Compute Adjusted Rand Index
ari = adjusted_rand_score(labels_kmeans, labels_hier)
print("Adjusted Rand Index (ARI):", ari)

# Compute Normalized Mutual Information
nmi = normalized_mutual_info_score(labels_kmeans, labels_hier)
print("Normalized Mutual Information (NMI):", nmi)

import matplotlib.pyplot as plt
import seaborn as sns

# Count the number of points in each cluster
cluster_counts = Final_Variables['Cluster_3'].value_counts().sort_index()

# Plot using barplot
plt.figure(figsize=(6,4))
sns.barplot(x=cluster_counts.index, y=cluster_counts.values, palette="Set2")
plt.xlabel("Cluster")
plt.ylabel("Number of Points")
plt.title("Cluster Sizes")
plt.show()

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns
#pca projection plot 
# Create a mapping: 0→1, 1→2, 2→3
cluster_labels = Final_Variables["Cluster_3"].map({0:1, 1:2, 2:3})

X_plot = Final_Variables.drop(columns=["Cluster_3"])

# PCA to 2 components
pca = PCA(n_components=2)
pca_proj = pca.fit_transform(X_plot)

plt.figure(figsize=(7,6))
sns.scatterplot(
    x=pca_proj[:, 0], 
    y=pca_proj[:, 1], 
    hue=cluster_labels,
    palette="Set2",
    s=20,
    alpha=0.7
)

plt.title("PCA Projection of Counties")
plt.xlabel("PCA Component 1")
plt.ylabel("PCA Component 2")
plt.legend(title="Cluster")
plt.show()

import matplotlib.pyplot as plt
#mean of within cluster comparison 
selected_vars = [
    "HEALTHCARE_PROVIDER_SPECIALTY_PPCA1",
    "HEALTHCARE_STRUCTURAL_PROVIDER_PPCA1",
    "HealthOutcomes_PPCA1",
    "Education_PPCA1",
    "HEALTH_BEH_BAD_PPCA1"
]

Final_Variables["Cluster_label"] = Final_Variables["Cluster_3"].map({0:1, 1:2, 2:3})

cluster_means = Final_Variables.groupby("Cluster_label")[selected_vars].mean()

ax = cluster_means.T.plot(kind='bar', figsize=(10,6))
plt.title("Cluster-wise Means of Main Contributors")
plt.xlabel("Variable")
plt.ylabel("Mean")

wrapped_labels = ['\n'.join(label.split('_')) for label in cluster_means.T.index]

plt.xticks(ticks=range(len(wrapped_labels)), labels=wrapped_labels, rotation=0)
plt.legend(title="Cluster")
plt.tight_layout()
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# heat map of main contributors 
selected_vars = [
    "HEALTHCARE_PROVIDER_SPECIALTY_PPCA1",
    "HEALTHCARE_STRUCTURAL_PROVIDER_PPCA1",
    "HealthOutcomes_PPCA1",
    "Education_PPCA1",
    "HEALTH_BEH_BAD_PPCA1"
]

# Mapping to shorter, readable labels
short_labels = {
    "HEALTHCARE_PROVIDER_SPECIALTY_PPCA1": "HC Specialty PC1",
    "HEALTHCARE_STRUCTURAL_PROVIDER_PPCA1": "HC Structural PC1",
    "HealthOutcomes_PPCA1": "Outcomes PC1",
    "Education_PPCA1": "Education PC1",
    "HEALTH_BEH_BAD_PPCA1": "Bad Behaviors PC1"
}

# Compute cluster means
cluster_means = Final_Variables.groupby("Cluster_3")[selected_vars].mean()

cluster_means_short = cluster_means.rename(columns=short_labels)

plt.figure(figsize=(10,6))
sns.heatmap(cluster_means_short, annot=True, cmap="YlGnBu", fmt=".2f")
plt.title("Heatmap of Cluster Centroids (Main Contributors)")
plt.xlabel("Variables")
plt.ylabel("Clusters")
plt.show()



